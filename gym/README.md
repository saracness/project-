# ğŸ“ Reinforcement Learning Gym
## A Textbook Approach to RL: From Theory to Practice

Welcome to the RL Gym! This is a **hands-on textbook** that teaches you Reinforcement Learning from scratch, with both theory and working code.

---

## ğŸ“š Table of Contents

### Part I: Foundations
1. [What is Reinforcement Learning?](#what-is-rl)
2. [Key Concepts: MDP, Rewards, Policies](#key-concepts)
3. [The RL Framework](#rl-framework)

### Part II: Tabular Methods
4. [Chapter 1: Q-Learning](tutorials/01_q_learning_gridworld.md)
5. [Chapter 2: SARSA and Expected SARSA](tutorials/02_sarsa.md)
6. [Chapter 3: Monte Carlo Methods](tutorials/03_monte_carlo.md)

### Part III: Deep Reinforcement Learning
7. [Chapter 4: Deep Q-Networks (DQN)](tutorials/04_deep_q_network.md)
8. [Chapter 5: Policy Gradients (REINFORCE)](tutorials/05_policy_gradient.md)
9. [Chapter 6: Actor-Critic Methods (A2C/A3C)](tutorials/06_actor_critic.md)
10. [Chapter 7: Proximal Policy Optimization (PPO)](tutorials/07_ppo.md)

### Part IV: Custom Environments
11. [Chapter 8: Building Custom Envs](tutorials/08_custom_environments.md)
12. [Chapter 9: MICROLIFE RL Environment](tutorials/09_microlife_rl.md)
13. [Chapter 10: Multi-Agent RL](tutorials/10_multi_agent.md)

### Part V: Advanced Topics
14. [Chapter 11: Model-Based RL](tutorials/11_model_based.md)
15. [Chapter 12: Curiosity & Exploration](tutorials/12_exploration.md)
16. [Chapter 13: Hierarchical RL](tutorials/13_hierarchical_rl.md)

---

## ğŸ¯ What is RL?

**Reinforcement Learning** is learning what to doâ€”how to map situations to actionsâ€”to maximize a numerical reward signal.

### The Core Problem

An **agent** interacts with an **environment**:
1. Agent observes **state** s
2. Agent takes **action** a
3. Environment returns **reward** r and new **state** s'
4. Repeat

The agent's goal: Find a **policy** Ï€ that maximizes **cumulative reward**.

### Example: Teaching a Robot to Walk

```
State (s):     Joint angles, velocities
Action (a):    Torques to apply to joints
Reward (r):    +1 for forward motion, -1 for falling
Policy (Ï€):    Function that maps states â†’ actions
Goal:          Learn Ï€ that makes robot walk forward
```

---

## ğŸ§  Key Concepts

### 1. Markov Decision Process (MDP)

An MDP is a tuple (S, A, P, R, Î³):
- **S**: Set of states
- **A**: Set of actions
- **P**: Transition probabilities P(s'|s,a)
- **R**: Reward function R(s,a,s')
- **Î³**: Discount factor (0 â‰¤ Î³ â‰¤ 1)

### 2. Policy

A policy Ï€ maps states to actions:
- **Deterministic**: Ï€(s) = a
- **Stochastic**: Ï€(a|s) = probability of action a in state s

### 3. Value Functions

**State Value Function V^Ï€(s)**:
```
V^Ï€(s) = E[R_t + Î³R_{t+1} + Î³Â²R_{t+2} + ... | S_t = s, Ï€]
```
Expected return starting from state s, following policy Ï€.

**Action Value Function Q^Ï€(s,a)**:
```
Q^Ï€(s,a) = E[R_t + Î³R_{t+1} + Î³Â²R_{t+2} + ... | S_t = s, A_t = a, Ï€]
```
Expected return from taking action a in state s, then following Ï€.

### 4. Bellman Equations

**Bellman Expectation Equation**:
```
V^Ï€(s) = Î£_a Ï€(a|s) Î£_{s',r} P(s',r|s,a) [r + Î³V^Ï€(s')]
```

**Bellman Optimality Equation**:
```
V*(s) = max_a Î£_{s',r} P(s',r|s,a) [r + Î³V*(s')]
Q*(s,a) = Î£_{s',r} P(s',r|s,a) [r + Î³ max_{a'} Q*(s',a')]
```

### 5. Exploration vs. Exploitation

**Exploration**: Try new actions to discover their rewards
**Exploitation**: Choose actions known to yield high rewards

**Îµ-greedy**: With probability Îµ, explore (random action); otherwise exploit (best action)

---

## ğŸ”§ The RL Framework

### Standard Environment Interface

```python
class Environment:
    def reset(self):
        """Reset environment, return initial state"""
        return state

    def step(self, action):
        """Take action, return (next_state, reward, done, info)"""
        return next_state, reward, done, info

    def render(self):
        """Visualize current state"""
        pass
```

### Standard Agent Interface

```python
class Agent:
    def select_action(self, state):
        """Choose action based on current state"""
        return action

    def learn(self, state, action, reward, next_state, done):
        """Update policy based on experience"""
        pass
```

---

## ğŸš€ Quick Start

### Example 1: Q-Learning on GridWorld (5 minutes)

```bash
cd gym/tutorials
python 01_q_learning_gridworld.py
```

**What you'll learn:**
- States, actions, rewards
- Q-table updates
- Îµ-greedy exploration
- Convergence to optimal policy

### Example 2: DQN on CartPole (15 minutes)

```bash
python 04_deep_q_network.py
```

**What you'll learn:**
- Neural network function approximation
- Experience replay
- Target networks
- Training stability

### Example 3: RL on MICROLIFE (30 minutes)

```bash
python 09_microlife_rl.py
```

**What you'll learn:**
- Custom environments
- Continuous action spaces
- Multi-objective rewards
- Biological realism

---

## ğŸ“Š Learning Path

### Beginner Track (Start Here!)
1. âœ… **GridWorld Q-Learning** â†’ Learn basics
2. âœ… **SARSA** â†’ Compare with Q-Learning
3. âœ… **CartPole DQN** â†’ Neural networks

### Intermediate Track
4. âœ… **Policy Gradients** â†’ Different approach
5. âœ… **Actor-Critic** â†’ Best of both worlds
6. âœ… **PPO** â†’ State-of-the-art

### Advanced Track
7. âœ… **Custom MICROLIFE Env** â†’ Real-world problems
8. âœ… **Multi-Agent RL** â†’ Competition/cooperation
9. âœ… **Model-Based RL** â†’ Planning

---

## ğŸ† Algorithms Implemented

### Tabular Methods (Discrete State/Action)
| Algorithm | Type | Complexity | Use Case |
|-----------|------|------------|----------|
| Q-Learning | Off-policy TD | Simple | Small discrete problems |
| SARSA | On-policy TD | Simple | Safe exploration |
| Monte Carlo | Episode-based | Medium | Episodic tasks |

### Deep RL (Continuous State)
| Algorithm | Type | Complexity | Use Case |
|-----------|------|------------|----------|
| DQN | Value-based | Medium | Discrete actions |
| REINFORCE | Policy gradient | Medium | Stochastic policies |
| A2C/A3C | Actor-critic | High | General purpose |
| PPO | Actor-critic | High | Stable training |
| DDPG | Actor-critic | High | Continuous control |
| SAC | Actor-critic | High | Robust performance |

---

## ğŸ“ Directory Structure

```
gym/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ envs/                        # Environment implementations
â”‚   â”œâ”€â”€ gridworld.py            # Simple GridWorld
â”‚   â”œâ”€â”€ cartpole.py             # Cartpole (OpenAI Gym compatible)
â”‚   â”œâ”€â”€ microlife_env.py        # MICROLIFE RL environment
â”‚   â””â”€â”€ base_env.py             # Base environment class
â”œâ”€â”€ agents/                      # RL agent implementations
â”‚   â”œâ”€â”€ q_learning.py           # Q-Learning agent
â”‚   â”œâ”€â”€ dqn.py                  # Deep Q-Network
â”‚   â”œâ”€â”€ ppo.py                  # Proximal Policy Optimization
â”‚   â””â”€â”€ base_agent.py           # Base agent class
â”œâ”€â”€ tutorials/                   # Step-by-step tutorials
â”‚   â”œâ”€â”€ 01_q_learning_gridworld.py
â”‚   â”œâ”€â”€ 04_deep_q_network.py
â”‚   â””â”€â”€ 09_microlife_rl.py
â”œâ”€â”€ examples/                    # Complete working examples
â”‚   â”œâ”€â”€ train_cartpole.py
â”‚   â”œâ”€â”€ train_microlife.py
â”‚   â””â”€â”€ compare_algorithms.py
â”œâ”€â”€ visualizations/              # Visualization tools
â”‚   â”œâ”€â”€ plot_training.py
â”‚   â”œâ”€â”€ render_policy.py
â”‚   â””â”€â”€ animate_episode.py
â””â”€â”€ cpp/                         # C++ implementations (performance)
    â”œâ”€â”€ q_learning.cpp
    â”œâ”€â”€ dqn.cpp
    â””â”€â”€ microlife_rl.cpp
```

---

## ğŸ“ Textbook Style Learning

Each tutorial follows this structure:

### 1. Theory Section
- Mathematical formulation
- Intuitive explanation
- Pseudocode

### 2. Implementation
- Python code (readable)
- C++ code (fast)
- Line-by-line explanation

### 3. Experiments
- Train agent
- Visualize learning curves
- Compare hyperparameters

### 4. Exercises
- Modify code
- Test understanding
- Explore variations

---

## ğŸ”¬ Example: Q-Learning

### Theory

**Q-Learning Algorithm**:
```
Initialize Q(s,a) arbitrarily
For each episode:
    Initialize state s
    For each step:
        Choose action a using Îµ-greedy policy from Q
        Take action a, observe r, s'
        Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
        s â† s'
```

### Python Implementation

```python
class QLearningAgent:
    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = alpha      # Learning rate
        self.gamma = gamma      # Discount factor
        self.epsilon = epsilon  # Exploration rate

    def select_action(self, state):
        # Îµ-greedy policy
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)  # Explore
        else:
            return np.argmax(self.Q[state])  # Exploit

    def learn(self, state, action, reward, next_state):
        # Q-Learning update
        target = reward + self.gamma * np.max(self.Q[next_state])
        self.Q[state, action] += self.alpha * (target - self.Q[state, action])
```

### Training Loop

```python
agent = QLearningAgent(n_states=100, n_actions=4)
env = GridWorld()

for episode in range(1000):
    state = env.reset()
    total_reward = 0

    while True:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.learn(state, action, reward, next_state)

        state = next_state
        total_reward += reward

        if done:
            break

    print(f"Episode {episode}: Reward = {total_reward}")
```

---

## ğŸ¯ Learning Objectives

By completing this gym, you will:

### Understand Theory
- âœ… Markov Decision Processes
- âœ… Bellman equations
- âœ… Policy vs. value iteration
- âœ… Temporal difference learning
- âœ… Function approximation

### Implement Algorithms
- âœ… Q-Learning
- âœ… Deep Q-Networks (DQN)
- âœ… Policy gradients
- âœ… Actor-critic methods
- âœ… PPO

### Apply to Real Problems
- âœ… Custom environments
- âœ… Biological systems (MICROLIFE)
- âœ… Multi-agent scenarios
- âœ… Continuous control

### Master Best Practices
- âœ… Hyperparameter tuning
- âœ… Debugging RL
- âœ… Visualization
- âœ… Reproducibility

---

## ğŸ“š Recommended Reading

### Textbooks
1. **Sutton & Barto** - "Reinforcement Learning: An Introduction" (2nd ed.)
   - THE classic RL textbook
   - Free online: http://incompleteideas.net/book/the-book-2nd.html

2. **Bertsekas** - "Reinforcement Learning and Optimal Control"
   - More mathematical approach

### Papers
1. **DQN**: Mnih et al. (2015) - "Human-level control through deep RL"
2. **PPO**: Schulman et al. (2017) - "Proximal Policy Optimization"
3. **AlphaGo**: Silver et al. (2016) - "Mastering the game of Go with deep RL"

### Online Courses
1. **DeepMind x UCL**: RL Lecture Series (YouTube)
2. **Berkeley CS285**: Deep Reinforcement Learning
3. **Stanford CS234**: Reinforcement Learning

---

## ğŸš€ Getting Started

### 1. Install Dependencies

```bash
# Python
pip install numpy matplotlib torch gym

# C++ (for high performance)
# SFML already installed for visualizations
```

### 2. Run Your First Tutorial

```bash
cd gym/tutorials
python 01_q_learning_gridworld.py
```

### 3. Follow the Textbook

Start with Chapter 1 and work your way through!

---

## ğŸ® Environments Available

### Classic Control
- **GridWorld** - Learn navigation
- **CartPole** - Balance a pole
- **MountainCar** - Climb a hill
- **Pendulum** - Swing up

### Custom Biological
- **MICROLIFE Survival** - Organism learns to survive
- **MICROLIFE Predator** - Learn to hunt
- **MICROLIFE Ecosystem** - Multi-agent dynamics

### Research
- **Multi-Agent** - Competition/cooperation
- **Sparse Rewards** - Hard exploration
- **Continuous Control** - Real-world robotics

---

## ğŸ“Š Performance Comparison

| Environment | Q-Learning | DQN | PPO | Training Time |
|-------------|------------|-----|-----|---------------|
| GridWorld 10x10 | âœ… 1 min | N/A | N/A | - |
| CartPole | N/A | âœ… 5 min | âœ… 3 min | Python |
| MICROLIFE | N/A | âœ… 30 min | âœ… 15 min | Python |
| MICROLIFE | N/A | âœ… 5 min | âœ… 3 min | C++ |

*C++ implementations are 6-10x faster than Python*

---

## ğŸ› Debugging RL

Common issues and solutions:

### 1. Agent Not Learning
- Check reward signal (is it too sparse?)
- Verify Q-updates are happening
- Plot Q-values over time
- Try lower learning rate

### 2. Unstable Training
- Reduce learning rate
- Increase batch size (DQN)
- Use target network (DQN)
- Normalize observations

### 3. Slow Convergence
- Tune exploration (Îµ)
- Adjust discount factor (Î³)
- Use reward shaping
- Increase network capacity (DQN)

---

## ğŸ¯ Next Steps

1. **Complete Tutorial 1** - Get your hands dirty with Q-Learning
2. **Implement from Scratch** - Don't just copy-paste!
3. **Experiment** - Change hyperparameters, see what happens
4. **Build Custom Environment** - Apply to your own problem
5. **Read Papers** - Understand state-of-the-art
6. **Contribute** - Share your implementations!

---

## ğŸ¤ Contributing

This is a living textbook! Contributions welcome:
- New algorithms
- Better explanations
- Bug fixes
- More examples

---

## ğŸ“„ License

Educational use - feel free to learn, modify, and share!

---

**Ready to learn RL? Start with [Tutorial 1: Q-Learning](tutorials/01_q_learning_gridworld.md)!** ğŸš€

---

*"The reward is the signal." - Richard Sutton*
